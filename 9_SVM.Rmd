---
title: "9_SVM"
author: "Park Ju ho"
date: '2022 6 15 '
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
mainfont: UnDotum
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

# spam data 불러오기
```{r}
setwd("D:/학교/4-1학기/데마입/R")

spam <- read.table('spam.txt', header=T, sep='\t')
str(spam)
spamTrain <- spam[spam$rgroup>=10,]
spamTest <- spam[spam$rgroup<10,]

spamVars <- setdiff(colnames(spam),list('rgroup','spam'))

```
# logistic
```{r}
spamFormula <- as.formula(paste('spam=="spam"',
                                paste(spamVars,collapse=' + '),sep=' ~ '))
spamModel <- glm(spamFormula,family=binomial(link='logit'),data=spamTrain)

spamTest$pred <- predict(spamModel,newdata=spamTest, type='response')

print(with(spamTest,table(y=spam,glPred=pred>=0.5)))
```

# svm
svm은 통계적 모형이라고 보기 힘들기에 결과를 보면 Support vector의 수 정도만 출력된다
kernel = 말 그대로 kernel 종류, vanilladot = 특별한 변환 없이 내적을 계산, rdfdot = Gaussian
cost = 제약 위배의 비용(작을 수록 시간이 오래 걸림)
prob.model = 분류를 위한 확률
cross = cross validation의 k 수

이유를 모르겠으나 학습이 진행되질 않음.....
```{r eval=FALSE}
library(kernlab)

library(kernlab)
spamFormulaV <- as.formula(paste('spam',paste(spamVars,collapse=' + '),sep=' ~ '))
svmM <- ksvm(spamFormulaV,data=spamTrain, kernel='rbfdot',C=10, prob.model=T, cross=5)
spamTest$svmPred <- predict(svmM,newdata=spamTest,type='response')
print(with(spamTest,table(y=spam,svmPred=svmPred)))
print(svmM)

head(predict(svmM,spamTest,type='probabilities' ))
```
# 비선형 svm
```{r}
#나선 모양의 데이터 생성
library(kernlab)
data(spirals) 

set.seed(1)
sc <- specc(spirals, centers = 2)   
s <- data.frame(x=spirals[,1],y=spirals[,2],class=as.factor(sc))

library('ggplot2')
ggplot(data=s) + geom_text(aes(x=x,y=y,label=class,color=class)) +
   coord_fixed() + theme_bw() + theme(legend.position='none')

set.seed(123)
s$group <- sample.int(100,size=dim(s)[[1]],replace=T)
sTrain <- subset(s,group>10)
sTest <- subset(s,group<=10)
```
<br><br>

선형 커널을 사용한 SVM으로 데이터를 분류
```{r}
library(e1071)
mSVMV <- svm(class~x+y,data=sTrain,kernel='linear',type='C-classification') 

mSVMV

sTest$predSVMV <- predict(mSVMV,newdata=sTest,type='response')  
print(with(sTest,table(y=class,svmPred=predSVMV)))
```
Support vector는 231개이고
총 32개의 데이터 중 7개를 틀린 것을 확인 할 수 있다

## hyperprameter tunning
gamma와 cost를 tune 함수를 이용하여 가장 best의 모형을 찾는 것
```{r}
tune.result<-tune(svm, class~x+y,data=sTrain,kernel='linear',type='C-classification', 
		range=list(gamma=seq(0,1,0.1),cost=2^(1:9)))

tune.result
```
그 결과gamma는 0 cosst는 2 일 때가 가장 좋다는 결론이 나옴

```{r}
ggplot() + geom_text(data=sTest,aes(x=x,y=y,label=predSVMV),size=12) +
   	geom_text(data=s,aes(x=x,y=y,label=class,color=class),alpha=0.7) +
   	coord_fixed() + theme_bw() + theme(legend.position='none') 

```
<br><br>

큰 글씨 = 예측값
=> 1:4개 2:3개를 틀렸다는 결과처럼 그래프를 보면 총 7개가 잘 못 분류되 어 있다

# 새로운 데이터로 해보는 svm
## new data
```{r}
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
```
```{r}
dat = data.frame(x, y = as.factor(y))
#y가 vector이기 떄문에 자동적으로 분류로 넘어감
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)

plot(svmfit, dat)#x = support vector
```
<br><br>

총 6개의 서포트 백터로 구성되어 있다

```{r}
make.grid = function(x, n = 75) {
  grange = apply(x, 2, range)
  x1 = seq(from = grange[1,1], to = grange[2,1], length = n) #grange = 외부인자를 받아와서 seq를 만드는 함수
  x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
  expand.grid(X1 = x1, X2 = x2)
}#점 찍어주는 함수

xgrid = make.grid(x)
xgrid[1:10,]

ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
```
<br><br>
상자친 서포트 벡터를 표시하여 svm 결과를 표시

```{r}
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])#결정경계 직선 그리그 위하여
beta0 = svmfit$rho

plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
```
<br><br>

결정경계와 plus,minnus plane을 추가한 그래프

# 비선형 svm의 다른 예제
```{r}
load(file = "ESL.mixture.rda")
names(ESL.mixture)

rm(x, y)
attach(ESL.mixture)
plot(x, col = y + 1)
```

```{r}
dat = data.frame(y = factor(y), x)
fit = svm(factor(y) ~ ., data = dat, scale = FALSE, kernel = "radial", cost = 5)
fit
```
총 103개의 백터와 radial kernel을 활용하여 svm 진행

```{r}
xgrid = expand.grid(X1 = px1, X2 = px2)
ygrid = predict(fit, xgrid)

plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)
points(x, col = y + 1, pch = 19)
```
<br><br>

값과 경계를 표현하면 이런 식으로 그래프가 나옴
```{r}
func = predict(fit, xgrid, decision.values = TRUE)
func = attributes(func)$decision

xgrid = expand.grid(X1 = px1, X2 = px2)
ygrid = predict(fit, xgrid)
plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)
points(x, col = y + 1, pch = 19)

contour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE)
contour(px1, px2, matrix(func, 69, 99), level = 0.5, add = TRUE, col = "blue", lwd = 2)
```
<br><br>

그 결졍경계를 표시하여 등고선으로 그림



























