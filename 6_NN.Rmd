---
title: "6_NN"
author: "Park Ju ho"
date: '2022 6 9 '
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
mainfont: UnDotum
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

# 사용 패키지
```{r}
library(nnet)
library(devtools)
library(scales)
library(clusterGeneration)
library(reshape)
library(neuralnet)
library(MASS)
```

# 순전파 신경망
iris data에 대한 순전파 신경망
파라미터 설명
size = 벡터의 길이 = 층의 수, 벡터 안의 값 = 노드 수
decay = 수렴 판단 기준으로 추정의 변화량이 이 숫자보다 작아지면 종료 = early stopping
maxit = epoch

```{r}
nn.iris = nnet(Species~., data = iris, size = c(2,2), rang=.1, decay = 5e-4, maxit=200)

summary(nn.iris)
```

즉 4 - 2 - 2의 신경망이 완성
절편을 포함하여 총 16개의 가중치를 계산함
i = input 즉 입력층 h = hidden 은닉층 o = 은닉층 b = 절편

# 은닉층의 수에 따라 오차 비교
```{r}
#은닉층의 수를 매개변수로 err 값을 계산해 주는 함수
test.err = function(h.size){
  ir = nnet(Species~., data=iris, size = h.size,
            decay = 5e-4, trace=F)
  y = iris$Species
  p = predict(ir, iris, type = "class")
  err = mean(y != p)
  c(h.size, err)
}
```

```{r}
out = t(sapply(2:10,FUN = test.err))
plot(out,type="b",xlab = "The number of Hidden units", ylab = "Test Error")
```
<br><br>
은닉 노드수가 늘어 날 수록 오차가 줄어드는 것을 확인 할 수 있다
하지만 과적합이 일어날 수 있으므로 Drop out이 필요하다

# 역전파 인공신경망
```{r}
net.iris = neuralnet(Species~.,hidden=c(2,2),data=iris,linear.output = F,stepmax=1e+10)
```
## sample을 하나씩 넣었을 때 찾아낸 가중치의 결과
```{r}
#net.iris$generalized.weights
```
결과값이 너무 길어서 출력은 하지 않음
NaN은 계산이 불가능한 경우, 가중치의 크기가 결국 표본별로 영향력의 척도라고 할 수 있으나 해석은 불가능

## 가중치의 초기치
```{r}
net.iris$startweights
```

## 인공신경망의 결과
```{r}
net.iris$result.matrix
```
reach.threshold는 beta가 일정 값에 도달하였을 때 마지막과 그 직전의 변화량을 보여줌
-.to.- = 각 노드에서 노드별 가중치를 보여줌

## 신경망 그래프
```{r}
plot(net.iris)
```
<br><br>
## 신경망을 이용한 예측
```{r}
compute(net.iris,iris)$net.result
```
<br><br>
출력 노드에서 나오는 확률값, 각 범주에 속하 확률을 제공

# 자녀 부모 정보에 대한 신경망 구축
```{r}
data(infert)
net.infert = neuralnet(case~parity + induced + spontaneous,hidden = c(20,20),data = infert, linear.output=F)
```

## 신경망 그래프
```{r}
plot(net.infert)
```
<br><br>

## generalized weights를 그래프로 표현
```{r}
head(net.infert$generalized.weights[[1]])#열의 수 = 입력변수의 수
par(mfrow=c(2,2))
#gwplot(net.infert, selected.covariate='age', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='parity', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='induced', min=-2.5, max=5)
gwplot(net.infert, selected.covariate='spontaneous', min=-2.5, max=5)
```
<br><br>

# 보스턴 집 값 예측에 신경망 활용
## 회귀를 활용하여 예측
```{r}
#결측치 확인
apply(Boston,2,function(x) sum(is.na(x)))

index <- sample(1:nrow(Boston),round(0.75*nrow(Boston)))
train <- Boston[index,]
test <- Boston[-index,]
lm.fit <- glm(medv~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)

MSE.lm
```
MSE가 20정도로 나옴
```{r}
maxs <- apply(Boston, 2, max) 
mins <- apply(Boston, 2, min)
scaled <- as.data.frame(scale(Boston, center = mins, scale = maxs - mins))#정규화
#center = 빼주는 값 scale = 나눠주는 값
train_ <- scaled[index,]
test_ <- scaled[-index,]
```
## 신경망
```{r}
n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
net.Boston <- neuralnet(f,data=train_,hidden=c(5,3), linear.output=T)

plot(net.Boston)
net.Boston$result.matrix

pr.nn <- compute(net.Boston,test_[,1:13])
pr.nn_ <- pr.nn$net.result*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)#예측값의 정규화 해제
test.r <- (test_$medv)*(max(Boston$medv)-min(Boston$medv))+min(Boston$medv)
MSE.nn <- sum((test.r - pr.nn_)^2)/nrow(test_)
c(MSE.lm,MSE.nn)
```
<br><br>
MSE를 비교해보면 신경망을 이용한 예측에서 훨씬 좋은 성과를 보여주고 있는 것을 확인 할 수 있다

## 그래프를 이용하여 회귀와 NN의 예측값 비교
```{r}
par(mfrow=c(1,2))
plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='NN',pch=18,col='red', bty='n')
plot(test$medv,pr.lm,col='blue',main='Real vs predicted lm',pch=18, cex=0.7)
abline(0,1,lwd=2)
legend('bottomright',legend='LM',pch=18,col='blue', bty='n', cex=.95)
```
<br><br>
그래프를 비교해보아도 NN이 훨씬 잘 예측하고 있는 것을 확인 할 수 있다
x축 = 실제 y축 = 예측 => 직선에 가까울수록 더 예측을 잘 한다고 할 수 있음
```{r}
plot(test$medv,pr.nn_,col='red',main='Real vs predicted NN',pch=18,cex=1)
points(test$medv,pr.lm,col='blue',pch=18,cex=1)
abline(0,1,lwd=2)
legend('bottomright',legend=c('NN','LM'),pch=18,col=c('red','blue'))
```

