---
title: "Ensemble"
author: "Park Ju ho"
date: '2022 6 11 '
output:
  pdf_document:
    latex_engine: xelatex
  word_document: default
mainfont: UnDotum
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```
# bagging

어군 탐지기 데이터
```{r}
library(tree)
library(mlbench)
data(Sonar)
str(Sonar)
```

## 데이터 전처리
```{r}
#클래스와 필요한 열 분리
clr = Sonar$Class; sonar = Sonar[,1:60]

#행렬로 변환
snx = as.matrix(sonar)

#범주를 0,1로 변환
sny = rep (1, 208); sny[which(clr == "R")] = 0
set.seed(120)

#test\validation 분리
lst = sample(208)
tr = lst[1:145]
val = lst[146:208]
da = data.frame(y=clr, xx=snx)
```

## tree 만들기
```{r}
#트리 생성
fgl.tr = tree(y ~ ., data=da[tr,], subset=tr)

#트리 k-fo
fgl.cv = cv.tree(fgl.tr, , prune.tree, K=10)
fgl.cv
```
size = terminal nodes의 수
dev = cv_error
k = cost-complexity parameter => 이 규제값을 활용하여 terminal nodes의 수를 결정


```{r}
#dev 즉 cv_err가 가장 낮은 값을 옵션으로 선택
opt = fgl.cv$k[which.min(fgl.cv$dev)]
opt

tt = prune.tree(fgl.tr, k=opt)
PP = predict(tt, da[val,], type="class")
mean(PP != clr[val])
```
0.3 정도의 오분류율을 보여주고 있음

## bagging 수행


```{r}
library(adabag)
#mfinal: m = 반복의 횟수 => 최종적으로 몇 번 반복 할 것 인지
fit.bag = bagging(y ~., data=da[-val,], mfinal=50)
# bagging의 오차율을 표현(오분유율)
predict.bagging(fit.bag, newdata=da[val,])$error
```
총 50개의 나무가 생성됨(mfinal = 50)

오차율이 0.23으로 감소한 것을 알 수 있다

### error이외에도 밑의 5가지 값들을 확인 가능
```{r}
predict.bagging(fit.bag, newdata=da[val,])$class
```
votes => 50개의 나무가 각 data에 대하여 투표한 결과
probs => votes를 비율로 나타낸 것
class => 실제 입력 y
sample => 145개의 표본을 복원추출한 결과들
importance => Feature의 중요도를 나타내는 것

# boosting
## 데이터 전처리
```{r}
wine = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", 
                  sep=",", header=T)
colnames(wine) = c("Type","Alcohol","Malic","Ash","Alcalinity","Magnesium","Phenols","Flavanoids","Nonflavanoids","Proanthocyanins","Color","Hue","Dilution","Proline")

#test,train 분리
lst = sample(nrow(wine)); tr = lst[1:100]; ts = lst[101:nrow(wine)]

#boosting에 적용시 Type을 factor로 바꿔줘야 함
ds = wine[tr,]; ds$Type = as.numeric(ds$Type)
ds$Type[ds$Type>1] = 0;
```
## boosting 수행

### adaboost
adaboost = boosting 방법
n.trees = 반복 횟수
cv.folds = k-fold validation
```{r}
library(gbm)

ds1.gbm = gbm(Type ~ Alcohol + Malic + Ash + Alcalinity + Magnesium +
                Phenols + Flavanoids + Nonflavanoids +
                Proanthocyanins +
                Color + Hue + Dilution + Proline,
              data=ds, distribution="adaboost", 
              n.trees=9000, cv.folds=5)

best1.iter = gbm.perf(ds1.gbm,method="cv")
```
<br><br>

여기서 iteration이 n.trees를 의미함
8000정도부터 exp가 급증하다가 9000이후로 조금 씩 내려옴
=> 변화 이후에 안정되는 상태가 n.trees의 가장 적절한 숫자라고 할 수 있기에 조금 더 큰 n.tree를 사용할 필요가 있음

```{r}
print(best1.iter)
ds1.gbm
```
There were 13 predictors of which 5 had non-zero influence.= 즉 13개의 변수 중 5개만 영향을 준다고 할 수 있음


### bernoulli
bernoulli 즉 일반적인 의사결정 나무를 사용 => 9000개의 나무이므로 Random Forest라고 볼 수 있음
```{r}
ds2.gbm = gbm(Type ~ Alcohol + Malic + Ash + Alcalinity + Magnesium +
                Phenols + Flavanoids + Nonflavanoids +
                Proanthocyanins +
                Color + Hue + Dilution + Proline,
              data=ds, distribution="bernoulli", n.trees=9000, cv.folds=5)
best2.iter = gbm.perf(ds2.gbm,method="cv")
#deviance는 작을 수로 좋음
print(best2.iter)
```
<br><br>

### 오차율 비교
```{r}

pp = predict(ds1.gbm,wine[ts,-1],type="response",n.trees=best1.iter)
#y값을 -1과 1로 변환
pyy = ifelse(wine$Type[ts]>1, -1, 1)

# gbm은 확률을 계산해주기에 0.5보다 크다면 1 작으면 -1이라고 할 수 있음
# 그렇기에 그 부호를 따서 오차율을 계산
mean(sign(pp-0.5) != pyy)

pp = predict(ds2.gbm,wine[ts,-1],type="response",n.trees=best2.iter)
pyy = ifelse(wine$Type[ts]>1, -1, 1)
mean(sign(pp-0.5) != pyy)
```

# Random Forest

## 데이터 전처리
```{r}
rm(list = ls())
setwd('D:/학교/4-1학기/데마입/R')
library(randomForest)
library(MASS)
library(gbm)

XY_tr = read.csv("LC_sample_tr.csv")
XY_ts = read.csv("LC_sample_ts.csv")
XY_tr = XY_tr[,-1]; XY_ts = XY_ts[,-1]
XY_tr[,4] = as.factor(XY_tr[,4]); XY_ts[,4] = as.factor(XY_ts[,4])
```

## RF 수행
ntree = 나무 개수 
```{r}
RF_res = randomForest(y ~ ., data=XY_tr, ntree=1000, Importance=TRUE)
summary(RF_res)
```
importanceSD = GINI 지수에 대한 표준 편차

```{r}
RF_res$importance
```

MeanDecreaseGini = GINI지수의 평균값을 말해줌(작을 수록 좋음)

```{r}
RF_res$confusion
```

class.error = 범주 0과 1에서 각각의 error율

```{r}
PP = predict(RF_res, XY_ts[,1:3])
mean(PP != XY_ts[,4])
```
7.5%정도의 오분류율을 가짐(Random이기 때문에 결과가 달라짐 하지만 크게 다르진 않음)

# Spam mail 데이터를 활용한 ensemble
## tree
### 데이터 전처리
```{r}
spamD <- read.table('https://raw.github.com/WinVector/zmPDSwR/master/Spambase/spamD.tsv',header=T,sep='\t')
head(spamD)

#rgoup을 이용하여 test와 train 데이터 구분
spamTrain <- subset(spamD,spamD$rgroup>=10)
spamTest <- subset(spamD,spamD$rgroup<10)

#setdiff = 차집합 을 활용하여 x와 y를 분리
spamVars <- setdiff(colnames(spamD),list('rgroup','spam'))
spamFormula <- as.formula(paste('spam=="spam"',     
                                paste(spamVars,collapse=' + '),sep=' ~ '))

#로드 우도 계산 함수
loglikelihood <- function(y, py) {      
  pysmooth <- ifelse(py==0, 1e-12,
                     ifelse(py==1, 1-1e-12, py))
  sum(y * log(pysmooth) + (1-y)*log(1 - pysmooth))
}

#정확도 측정 함수
accuracyMeasures <- function(pred, truth, name="model") {   
  dev.norm <- -2*loglikelihood(as.numeric(truth), pred)/length(pred)#Deviance의 형태    
  ctable <- table(truth=truth,
                  pred=(pred>0.5))
  accuracy <- sum(diag(ctable))/sum(ctable)
  precision <- ctable[2,2]/sum(ctable[,2])
  recall <- ctable[2,2]/sum(ctable[2,])
  f1 <- 2*precision*recall/(precision+recall)
  data.frame(model=name, accuracy=accuracy, f1=f1, dev.norm)
}
```

### tree 수행
```{r}
library(rpart)    
treemodel <- rpart(spamFormula , spamTrain)
```

### 평가
```{R}
accuracyMeasures(predict(treemodel, newdata=spamTrain), 
                 spamTrain$spam=="spam",
                 name="tree, training")
accuracyMeasures(predict(treemodel, newdata=spamTest), 
                 spamTest$spam=="spam",
                 name="tree, test")

```
<br><br>

train의 결과를 보았을 때 상당히 학습 잘 된 것을 알 수 있음.
test의 결과를 보았을 때 대략 87의 정확도를 보여주고 있음

## bagging
### bootstrap
```{r}
#총 행의 수 추출
ntrain <- dim(spamTrain)[1]
n <- ntrain                  
ntree <- 100

#부츠트랩 과정
#ntree = 100 즉 나무의 개수를 100개 만들 것이기 때문에 표본 역시 100개를 생산
samples <- sapply(1:ntree,       
                  FUN = function(iter)
                  {sample(1:ntrain, size=n, replace=T)})#부츠트랩이기에 replace=T

#bagging을 이용한 예측 함수
#나무의 개수만큼 반복하여 평균을 냄(혹은 보팅)
predict.bag <- function(treelist, newdata) {    
  preds <- sapply(1:length(treelist),
                  FUN=function(iter) {
                    predict(treelist[[iter]], newdata=newdata)})
  predsums <- rowSums(preds)
  predsums/length(treelist)
}

```
### tree 생성
```{r}
treelist <-lapply(1:ntree,          
                  FUN=function(iter)
                  {samp <- samples[,iter];
                  rpart(spamFormula, spamTrain[samp,])})
```
### 평가
```{r}
accuracyMeasures(predict.bag(treelist, newdata=spamTrain),      
                 spamTrain$spam=="spam",
                 name="bagging, training")

accuracyMeasures(predict.bag(treelist, newdata=spamTest),
                 spamTest$spam=="spam",
                 name="bagging, test")
```
<br><br>
test 데이터의 결과를 보았을 때 그냥 트리보다 정확도가 향상한 것을 확인 할 수 있음

## RandomForest
```{r}
set.seed(12345)   

fmodel <- randomForest(x=spamTrain[,spamVars],  
                       y=factor(spamTrain$spam),
                       ntree=100,  
                       nodesize=7,#Tree의 node수를 설정(간단히 2진 Tree라면 깊이가 2인 Tree가 생성)     
                       importance=T)  

summary(fmodel)
fmodel$err.rate

accuracyMeasures(predict(fmodel, 
                         newdata=spamTrain[,spamVars], type='prob')[,'spam'],
                 spamTrain$spam=="spam",name="random forest, train")

accuracyMeasures(predict(fmodel,
                         newdata=spamTest[,spamVars],type='prob')[,'spam'],
                 spamTest$spam=="spam",name="random forest, test")
```
<br><br>
정확도가 더 개선된 것을 확인 할 수 있다

### 변수별 중요도
```{r}
varImp <- importance(fmodel)                
varImp[1:10, ]

varImpPlot(fmodel, main="varImpPlot")
```
<br><br>
변수별 중요도를 파악 할 수 있음

MeanDecreaseAccuracy = 분류 정확도를 개선하는데 기여한 정도 => 높을 수록 좋음
MeanDecreaseGini = 노드 불순도 개선에 기여한 정도 => 높을 수록 좋음






























